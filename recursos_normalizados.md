# Recursos de AI Safety & Governance (formato unificado)

Estructura consistente para todo el documento: cada entrada incluye resumen corto, publico objetivo, formato/modalidad, aplicacion/fechas y notas si aplica. Se usa una sola tipografia en el futuro diseño (sugerido: Space Grotesk o cualquier sans limpia).

## Cursos y bootcamps
### BlueDot Impact (online)
- Resumen: Cursos gratuitos sobre fundamentos de AI safety, alineamiento y gobernanza; opciones autoguiadas o con cohortes.
- Publico: Personas nuevas en AI safety sin requisito tecnico inicial.
- Formato: Online; desde 2 h autoguiado (Future of AI) hasta 5 semanas con seminarios.
- Aplicacion: Inscripciones abiertas y recurrentes en el sitio de BlueDot.

### Alignment Research Engineer Accelerator (ARENA) – Londres
- Resumen: Bootcamp de 4–5 semanas para pasar de programador fuerte a ingeniero de alineamiento.
- Publico: Estudiantes o profesionales con solidas bases en Python y matematicas.
- Formato: Presencial en Londres; viaje, hospedaje y comidas cubiertos.
- Aplicacion: Actualmente cerrada; seguir el formulario de interes de ARENA.

### ML Safety Course (Center for AI Safety) – online
- Resumen: Curso virtual de 8 semanas sobre robustez, monitoreo, alineamiento y seguridad sistemica.
- Publico: Personas con base en deep learning/ML que quieran pasar a investigacion en safety.
- Formato: Online, ~5 h/semana; autoinscripcion.
- Aplicacion: Sin proceso de admision; disponible en linea.

### ML4Good Bootcamps
- Resumen: Bootcamp intensivo de 10 dias con formacion tecnica y de gobernanza, mas proyecto final.
- Publico: Personas con experiencia tecnica interesadas en AI safety.
- Formato: Presencial; incluye sesiones de codigo, ponencias y capstone de 2.5 dias.
- Aplicacion: Ciclos periodicos; revisar convocatorias.

## Upskilling en ML (recomendaciones)
- Nivel intro: Stanford Machine Learning; Carnegie Mellon (Spring 2023 lectures).
- Cursos por area: Vision (UMich Deep Learning for CV); NLP (Stanford NLP con DL); RL (Stanford RL o Berkeley Deep RL).
- Comprension avanzada: Deep Multi-Task and Meta Learning; Deep Unsupervised Learning; Optimizacion/Convex Optimization (Boyd).
- Otros utiles: Cursos de Roger Grosse; Deep Learning Theory (Preetum Nakkiran); paper de Usman Anwar (2025).

## Agendas de investigacion
- Technical AI Safety: Foundational Challenges in Assuring Alignment and Safety of Large Language Models; Open Problems in Mechanistic Interpretability.
- AI Governance: A Collection of AI Governance Research Ideas (2024).
- Technical AI Governance: Open Problems in Technical AI Governance; Open Problems and Concrete Projects in Evals.

## Fellowships tecnicos (y technical governance)
### ML Alignment Theory Scholars (MATS) – Berkeley/global
- Resumen: 10 semanas de investigacion guiada en alineamiento/interpretabilidad con mentores.
- Publico: Personas con buena comprension del panorama de alineamiento (ej. egresados de AGI Safety Fundamentals) y experiencia previa de investigacion.
- Formato: Hub en Berkeley con apoyo para remoto; AI Safety Support ofrece estipendio (~USD 12k/10 semanas).
- Aplicacion: Ciclos de invierno/verano; invierno 2025 cerrado.

### Pivotal Research Fellowship – Londres
- Resumen: Fellowship presencial de 9 semanas (antes CHERI) enfocado en reducir riesgos catastróficos via proyectos en safety, gobernanza o bioseguridad.
- Publico: Early-career con interes fuerte en riesgo de IA; no requiere gran experiencia previa.
- Formato: Presencial con estipendio (~GBP 5k) y soporte de viaje/hospedaje.
- Aplicacion: Cohortes de verano/invierno; revisar proxima fecha.

### Supervised Program for Alignment Research (SPAR) – remoto
- Resumen: Programa virtual de 3 meses en equipos con mentores para proyectos de alineamiento o gobernanza.
- Publico: Perfil tecnico o de ciencias sociales; compromiso ~10 h/semana.
- Formato: Remoto, voluntario; reuniones con mentor y entregables.
- Aplicacion: Ciclos recurrentes (primavera/verano).

### PIBBS Fellowship – US/UK
- Resumen: Proyectos en la interseccion de su area y AI safety, con mentorias dedicadas.
- Publico: PhD/postdocs (o experiencia equivalente) en sistemas complejos, comportamiento inteligente, matematicas, filosofia o ingenieria.
- Formato: Hibrido; grupo de lectura remoto 8 semanas + retiro y coworking presencial; estipendio ~USD 3,000/mes.
- Aplicacion: Cohortes periodicas; revisar sitio.

### London AI Safety Research Labs (LASR Labs) – Londres
- Resumen: 13 semanas en equipos de 3–4 para “aprender haciendo” y publicar investigacion.
- Publico: Investigadores tecnicos con base solida en ML/CS (PhD en curso o similar).
- Formato: Tiempo completo presencial; estipendio (~GBP 11k) + viaje, comida y oficina.
- Aplicacion: Cohortes invierno/verano; invierno 2026 cerrada.

### Apart Labs Fellowship – remoto
- Resumen: Incubadora de 3–6 meses para madurar ideas de proyectos (a menudo desde hackathons) hasta publicaciones.
- Publico: Personas con habilidades tecnicas solidas demostradas en sprints de Apart.
- Formato: Remoto, con mentoria, GPUs y apoyo de gestion.
- Aplicacion: Invitacion continua segun rendimiento en sprints.

### Impact Academy Global AI Safety Fellowship – remoto/hibrido
- Resumen: Fellowship financiado 3–6 meses con bootcamp inicial y colocacion en CHAI, Conjecture, FAR AI, UK AI Safety Institute u otros.
- Publico: Programadores o investigadores con experiencia en ML/soft dev que quieran pivotar a AI safety.
- Formato: Tiempo completo; preferencia presencial en la organizacion anfitriona (hibrido posible).
- Aplicacion: Convocatorias periodicas.

### Algoverse AI Safety Fellowship – remoto
- Resumen: 12 semanas online; equipos de 3 con mentores (Apollo, FAR.AI, MATS, UW, Oxford) para publicar en talleres EMNLP/NeurIPS/EACL.
- Publico: Estudiantes y profesionales con experiencia fuerte en programacion y ML/IA.
- Formato: Remoto, dedicacion 25+ h/semana; gratuito.
- Aplicacion: Cohortes periodicas.

### Anthropic Fellows Program – remoto
- Resumen: 6 meses financiados para proyectos de safety (robustez, oversight escalable, etc.) con mentores de Anthropic.
- Publico: Ingenieros e investigadores que quieran pivotar a AI safety.
- Formato: Remoto full-time; coworking opcional en San Francisco o Londres.
- Aplicacion: Ciclos anunciados por Anthropic.

## Fellowships de politica y gobernanza
### RAND Technology and Security Policy Fellowship – US/UK remoto/presencial
- Resumen: 1–3 anos para investigacion independiente en tecnologia emergente y politica de seguridad con mentorias RAND.
- Publico: Analistas de politica desde pregrado hasta mid-career; requiere elegibilidad de seguridad en US/UK.
- Formato: Flexible, remoto o en oficinas RAND; salario/estipendio competitivo con beneficios.
- Aplicacion: Ingreso continuo (revision trimestral).

### Horizon Fellowship – Washington DC
- Resumen: Fellowship de servicio publico colocando fellows en agencias, Congreso o think tanks 6–24 meses sobre gobernanza de IA.
- Publico: Profesionales early/mid-career con experiencia en IA o biotec; requiere autorizacion de trabajo US.
- Formato: Presencial DC; 1 ano renovable (gobierno) o terminos de 6 meses (think tank).
- Aplicacion: Cohortes segun disponibilidad.

### Institute for AI Policy & Strategy (IAPS) Fellowship – DC/remoto
- Resumen: 3 meses financiados para proyectos de gobernanza de modelos frontera; incluye coaching y networking.
- Publico: Profesionales de gobierno, derecho, tecnologia, academia; tracks Fellow/Senior.
- Formato: Tiempo completo; 2 semanas presenciales en DC + resto remoto; estipendio aprox. USD 15k/22k.
- Aplicacion: Convocatorias periodicas.

### Law & AI Institute Summer Fellowships – remoto con semanas presenciales
- Resumen: Fellowships pagados (US y EU) de 8–12 semanas en derecho y politica de IA, con semana intensiva in-person.
- Publico: Estudiantes de derecho, profesionales y academicos interesados en gobernanza; no se requiere experiencia previa en IA.
- Formato: Remoto-primero; semana en DC (US) o Bruselas/Cambridge (EU); estipendio ~USD 1,500/semana o ~EUR 1,200/semana.
- Aplicacion: Convocatorias anuales; revisar LawAI.

### GovAI Summer/Winter Fellowships – Londres
- Resumen: 3 meses para disenar y ejecutar un proyecto de investigacion en gobernanza con doble mentoria.
- Publico: Early-career en politicas publicas, derecho, economia, CS u otras ciencias sociales.
- Formato: Tiempo completo; presencial recomendado en Londres (desk y comidas); estipendio ~GBP 11k + apoyo de viaje.
- Aplicacion: Ciclos de verano/invierno.

### Talos Fellowship – Bruselas/Remoto
- Resumen: Programa de 7 meses para acelerar carreras en politica de IA en Europa; incluye curso fundamental, cumbre en Bruselas y colocacion en think tank.
- Publico: Graduados (preferencia UE) en politica, ML/CS, economia, derecho u afines; se considera talento no-UE con 3+ anos de experiencia.
- Formato: Track formacion (9 semanas online + cumbre de 1 semana) o track con colocacion 4–6 meses; estipendio ~EUR 2,000/mes en colocacion.
- Aplicacion: Segun convocatoria; apoyo a busqueda laboral opcional.

### Tarbell Fellowship – ubicacion segun newsroom
- Resumen: Fellowship de 1 ano para periodismo en tecnologias emergentes; 9 meses en newsroom (Wired, Washington Post, etc.) + 3 meses de estudio.
- Publico: Periodistas con 0–5 anos de experiencia o profesionales cambiando a periodismo de riesgo tecnologico.
- Formato: Principalmente presencial en la redaccion asignada; estipendio anual ~USD 50k + viajes de formacion.
- Aplicacion: Cohortes anuales.

## Tableros y asesorias de oportunidades
- 80,000 Hours Job Board: Lista curada de vacantes de alto impacto; ofrece asesorias 1:1.
- Fellowship masterlist: Catalogo amplio de fellowships en AI safety, tech policy y ciberseguridad.
- SuccessIf: Career advising enfocado en AI safety.
- Database of US Policy Fellowships: Buscador de fellowships de politica en EE. UU.
- Arkose Opportunities, EA Opportunity Board, AIsafety.training (cronograma de ventanas), Probably Good (job board adicional).

## Paquete de salida y siguientes pasos
- Accion rapida: proyectos paralelos divertidos, inscribirse a BlueDot, participar en Alignment Jams, aplicar a EAG(x).
- Hallar trabajo/practicas: tableros de EA y 80k.
- Resumen de camino: mantenerse informado (newsletters, eventos, cuentas X), formacion (cursos/fellowships), auto-upskilling, hacer cosas (grupos locales, hackathons, tesis, pruebas baratas), emprendimiento, planeacion de carrera (80k guias, teoria del cambio, proyectos prioritarios), hablar con un asesor, conferencias (EAG/EAGx), checklist de AI Safety Community.

## Programas de investigacion y upskilling (lista rapida)
- SPAR, ARENA, MATS, AI Safety Training (calendario), Talos Fellowship, AI Safety Camp, MARS, Pivotal, ERA, Astra Fellowship, RAND Tech & Security, GovAI Fellowship, TASP, Future Impact Group, PIBBS Summer Fellowship.

## Recursos de aprendizaje
- Conceptual AI Safety: AI Safety Textbook; Introduction to AI Safety, Ethics, and Society; AI Safety Fundamentals Curriculum.
- Machine Learning: ARENA course content; Deep Learning Foundations and Concepts (Bishop); Neuromatch Academy; Jacob Hilton DL Curriculum; Llama3 from scratch.

## Mantenerse al dia
- Newsletters: AI Safety Newsletter; Impact AI (Jack Clark); Transformer; Don’t Worry About the Vase; EU AI Act Newsletter; AI Lab Watch.
- Twitter/X: Lista de mejores cuentas (x.com/i/lists/1806265653503930395); lista de Alejo (AcelasAlejandro/following); sugerencia: usar LeechBlock NG para limitar tiempo.
- YouTube/Podcast: AI Explained; Dwarkesh Podcast; Robert Miles.

## 80,000 Hours career reviews (lecturas clave)
- AI Safety Technical Research; AI Governance and Policy; Information security in high-impact areas; Machine Learning PhDs; China-related AI safety and governance paths.

## Comunidades y proyectos
- Oportunidad boards (ver arriba) + AIsafety.training timeline.
- Listas de personas interesadas: FLI Community; firmantes de statements; FHI Open Letter; AI Watch; tabla interna (no compartir).
- Organizar grupo local: 1–3 personas bastan; actividades (papers, ejercicios, presentaciones, hackathons Apart); contactar profes; ofrecer charlas en grupos EA.
- Proyectos individuales: elegir tema, producir entregable (blog, LessWrong, Alignment Forum); inspirarse en agendas de Timaeus, CLR, posts con etiqueta “Research Agenda”, AI Safety Ideas DB, tareas de seleccion MATS.
- Voluntariado: Alignment.dev proyectos unibles.

## Emprendimiento y financiamiento
- Ideas/recursos: Career review “Founder of new projects tackling top problems”; programa Def/acc de Entrepreneur First; articulos sobre startups de AI safety; ideas de orgs con animo de lucro.
- Que buscan los financiadores: Long-Term Future Fund; Future of Life Institute; Survival and Flourishing Fund (3 streams); Open Philanthropy RFPs (technical, evals, governance, fieldbuilding); YC y otros VCs interesados.
- Consejos generales (Mick): enfocate donde se este fallando; prioriza problemas antes que soluciones; lee planes de alineamiento (The Narrow Path, The Compendium, Live Agendas in Alignment, What everyone in AI Governance is working on); usa guias para priorizar proyectos y teoria del cambio.
- Funding rapido: aisafety.com/funding; Open Philanthropy Career Transition Funding.

## Meta-recursos clave
- The Map of AI Safety; Resources – BlueDot Impact; Join AI Safety Community – Quick Wins; AI Safety Courses (lista de cursos); Less Wrong’s list para entrar a AI safety; AI Safety Support: Lots of Links.

## PIBBSS: recomendados extra
- Enlaces utiles: aisafety.training (y aisafety.com/events-and-training), aisafety.careers, aisafety.world, aisafety.quest, aisafety.info.
- Fellowships/programas: MATS; AGI Safety Fundamentals (8 semanas + capstone); AI Safety Camp; SERI y CHERI summer internships; GovAI fellowship; Legal Priorities Project; SFI-CI y otros SFI; OpenPhil AI Fellowship (PhD); Rethink Priorities fellowships/internships; UChicago X-risk Fellowship (10 semanas); Foresight Fellowship (1 ano de networking).
- Eventos: Human-aligned AI Summer School (Praga).
- Lecturas: Key Phenomena in AI Risk; AI alignment resources (Victoria Krakovna); PIBBSS resources (talks y biblioteca); AI Alignment Forum.
- Otros: Effective Thesis (coaching/IA chatbot); APART Labs hackathons.
